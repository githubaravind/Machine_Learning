{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T04:14:38.258488Z",
     "start_time": "2021-08-01T04:14:38.230538Z"
    }
   },
   "source": [
    "# Full understanding The Bias-Variance Tradeoff\n",
    "\n",
    "\n",
    "https://medium.com/swlh/the-bias-variance-tradeoff-f24253c0ab45\n",
    "    \n",
    "https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229\n",
    "    \n",
    "https://elitedatascience.com/bias-variance-tradeoff\n",
    "    \n",
    "    \n",
    "https://searchenterpriseai.techtarget.com/feature/6-ways-to-reduce-different-types-of-bias-in-machine-learning\n",
    "    \n",
    "https://towardsdatascience.com/contents-9b2e49f49fe9\n",
    "    \n",
    "https://machinelearningmastery.com/how-to-reduce-model-variance/\n",
    "    \n",
    "    \n",
    "https://www.section.io/engineering-education/ensemble-bias-var/\n",
    "    \n",
    "    \n",
    "https://www.cs.cmu.edu/~wcohen/10-601/bias-variance.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The types of errors in the prediction of a model are:\n",
    "* Bias error\n",
    "* Variance error\n",
    "* Irreducible error\n",
    "\n",
    "The predictive error of a model can be calculated as seen below:\n",
    "\n",
    "`Prediction error = (Bias error)^2 + Variance error + Irreducible error`\n",
    "\n",
    "![](./i/1_hb8F9jMk0UyYcS3jsyD8sg.png)\n",
    "If you guessed A has high Bias and B has high Variance, you’re right.\n",
    "\n",
    "\n",
    "![](./i/image43.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Error:\n",
    "When a Machine Learning model is unable to capture the true relationship between the features and target of the data, we have an error called Bias. OR Pays very little attention to the training data and oversimplifies the model. \n",
    "\n",
    "The Machine Learning model makes assumption based on the available data. If the assumption is too simple, then the model may not be able to accurately account for the relationship between the features and target of the data thereby producing inaccurate predictions.\n",
    "\n",
    "Mathematically, Bias can be defined as the difference between the Predicted values and the Expected values.\n",
    "\n",
    "![](./i/1_1R4Btn9TRhksPuIwjZTZ0g.png)\n",
    "\n",
    "Linear models such as Linear Regression and Logistic Regression which make simple assumptions have high bias. While models such as Decision Trees and Support Vector Machines have low bias.\n",
    "\n",
    "## Underfitting \n",
    "occurs when the model cannot accurately fit the training data and therefore performs poorly on training data.\n",
    "\n",
    "## Reducing Bias\n",
    "**1. Change the model(Choose the correct learning model):**\n",
    "One of the first stages to reducing Bias is to simply change the model. As stated above, some models have High bias while some do not. Do not use a Linear model if features and target of your data do not in fact have a Linear Relationship.\n",
    "\n",
    "**2. Ensure the Data is truly Representative (Use the right training dataset ):**\n",
    "\n",
    "Ensure that the training data is diverse and represents all possible groups or outcomes. In the event of an imbalanced dataset, use weighting or penalized models. There has been discussion on the poor accuracy of facial recognition models in identifying people of color. One possible source of such error is that the training dataset was not diverse and the model did not have enough training data to clearly identify persons of color.\n",
    "\n",
    "**3. Parameter tuning: **\n",
    "\n",
    "This requires an understanding of the model and model parameters. Algorithms documentations are a good place to start. Every model has a list of parameters which it takes as inputs. Tweaking these parameters may give you the desired results. You can also build your own algorithms from scratch.\n",
    "\n",
    "**4. Perform data processing mindfully** \n",
    "\n",
    "Machine intelligence involves three types of data processing: pre-processing, in-processing, and post-processing.  When you prepare datasets in pre-processing, bias can creep in during formatting before it is fed in the neural network. Any data that could introduce a bias should be excluded in this step. With in-processing, the data is manipulated as it passes through the neural network itself – so, the weighting of the neural nodes must be correct to prevent a biased output. Finally, ensure there is no bias when interpreting data for human-readable consumption in the post-processing stage. \n",
    "**5. Monitor real-world performance across the ML lifecycle **\n",
    "\n",
    "No matter how carefully you choose the learning model or vet the training data, the real-world can throw up unexpected challenges. It is important to not consider any ML model as “trained” and finalized, not requiring any further monitoring. Also, try and use real-world data for testing ML wherever possible so that bias can be detected and corrected before it creates a situation affecting human lives negatively.\n",
    "\n",
    "**6. Make sure that there are no infrastructural issues:**\n",
    "\n",
    "Apart from data and the human factor, the infrastructure itself could cause bias. For example, if you rely on data collected via electronic or mechanical sensors, then equipment problems can introduce bias. This is often the hardest type of bias to detect and needs careful consideration, with investment in the latest digital and technology infrastructure.  These five best practices should form the starting point in the discussion around bias in machine learning. \n",
    "\n",
    "## Bias is as a result of over simplified model assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance\n",
    "Pays too much attention to training data and does not generalize on the data.\n",
    "Variability of a model prediction for a given data point. We can build the model multiple\n",
    "times, so the variance is how much the predictions for a given point vary between different realizations of the model.![]\n",
    "\n",
    "## Overfitting \n",
    "occurs when a model has high variance and low bias. When a model fits too well with the training dataset such that it captures noise, it is said to have Overfit the training data. This will negatively impact the predictive power of the model. \n",
    "\n",
    "If our model returns a high accuracy on training data but performs poorly on testing data, we can denote that the model has fit too closely to the training data and can therefore not generalize on new data.\n",
    "## You can measure both types of variance in your specific model using your training data.\n",
    "\n",
    "### Measure Algorithm Variance: \n",
    "The variance introduced by the stochastic nature of the algorithm can be measured by repeating the evaluation of the algorithm on the same training dataset and calculating the variance or standard deviation of the model skill.\n",
    "### Measure Training Data Variance: \n",
    "The variance introduced by the training data can be measured by repeating the evaluation of the algorithm on different samples of training data, but keeping the seed for the pseudorandom number generator fixed then calculating the variance or standard deviation of the model skill.\n",
    "\n",
    "## Reducing Variance Error\n",
    "\n",
    "**1. Ensemble Learning:**\n",
    "\n",
    "A good way to tackle high variance is to train your data using multiple models. Ensemble learning is able to leverage on both weak and strong learners in order to improve model prediction. In fact, most winning solutions in Machine Learning \n",
    "competitions make use of Ensemble Learning.\n",
    "\n",
    "**2. Ensemble Parameters from Final Models.**\n",
    "\n",
    "**3. Increase Training Dataset Size: **\n",
    "\n",
    "This sounds tricky. Why add more data when the variance is high? More data increases the data to noise ratio which reduces the variance of the model. Also, when the model has more data, it is better able to come up with a general rule which will also apply to new data.\n",
    "\n",
    "**4. Decrease regularization :**\n",
    "\n",
    "regularization is the process of adding information (an additional penalty ) in order to solve an ill-posed problem or to prevent overfitting. Regularization makes the parameter values small and this prevents overfitting. Later in the post, we’ll see why does this work. Regularization is typically used to reduce the variance with a model by applying a penalty to the input parameters with the larger coefficients. There are a number of different methods, such as L1 regularization, Lasso regularization, dropout, etc., which help to reduce the noise and outliers within a model. However, if the data features become too uniform, the model is unable to identify the dominant trend, leading to underfitting. By decreasing the amount of regularization, more complexity and variation is introduced into the model, allowing for successful training of the model.\n",
    "\n",
    "## Variance occurs when the assumptions are too complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Best: low Variance, low bias\n",
    "High bias (no attention to detail) : \n",
    "1. Underfitting \n",
    "2. Overly-simplified Model \n",
    "3. High error on both test and train data\n",
    "\n",
    "![](./i/image12.png)\n",
    "\n",
    "## high Variance (too much attention to train):\n",
    " Overfitting, Low error on train data and high on test, Starts modelling the noise in the input\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is Bias Variance Tradeoff?\n",
    "\n",
    "If our model is too simple and has very few parameters then it may have high bias and low variance. On the other hand if our model has large number of parameters then it’s going to have high variance and low bias. So we need to find the right/good balance without overfitting and underfitting the data.\n",
    "\n",
    "**This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can’t be more complex and less complex at the same time.**\n",
    "\n",
    "![](./i/image30.png)\n",
    "\n",
    "\n",
    "### Reason 1:\n",
    "Low variance (high bias) algorithms tend to be less complex, with simple or rigid underlying structure.\n",
    "\n",
    "They train models that are consistent, but inaccurate on average.\n",
    "These include linear or parametric algorithms such as regression and naive Bayes.\n",
    "\n",
    "\n",
    "\n",
    "### Reason 2:\n",
    "On the other hand, low bias (high variance) algorithms tend to be more complex, with flexible underlying structure.\n",
    "\n",
    "They train models that are accurate on average, but inconsistent.\n",
    "These include non-linear or non-parametric algorithms such as decision trees and nearest neighbors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For example: \n",
    "    \n",
    "Voting Republican - 13 Voting Democratic - 16 Non-Respondent - 21 Total - 50\n",
    "The probability of voting Republican is 13/(13+16), or 44.8%. We put out our press release that the\n",
    "Democrats are going to win by over 10 points; but, when the election comes around, it turns out they\n",
    "lose by 10 points. That certainly reflects poorly on us. Where did we go wrong in our model?\n",
    "## Bias scenario's: \n",
    "using a phonebook to select participants in our survey is one of our sources of bias.\n",
    "By only surveying certain classes of people, it skews the results in a way that will be consistent if we repeated the entire model building exercise. Similarly, not following up with respondents is another source of bias, as it consistently changes the mixture of responses we get. On our bulls-eye diagram, these move us away from the center of the target, but they would not result in an increased scatter of estimates.\n",
    "## Variance scenarios: \n",
    "the small sample size is a source of variance. If we increased our sample size, the results would be more consistent each time we repeated the survey and prediction. The results still might be highly inaccurate due to our large sources of bias, but the variance of predictions will be reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A proper machine learning workflow includes:\n",
    "\n",
    "1. Separate training and test sets\n",
    "2. Trying appropriate algorithms (No Free Lunch)\n",
    "3. Fitting model parameters\n",
    "4. Tuning impactful hyperparameters\n",
    "5. Proper performance metrics\n",
    "6. Systematic cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfitting\n",
    "Learn how to avoid underfitting, so that you can generalize data outside of your model accurately.\n",
    "What is underfitting?\n",
    "Underfitting is a scenario in data science where a data model is unable to capture the relationship between the input and output variables accurately, generating a high error rate on both the training set and unseen data. It occurs when a model is too simple, which can be a result of a model needing more training time, more input features, or less regularization. Like overfitting, when a model is underfitted, it cannot establish the dominant trend within the data, resulting in training errors and poor performance of the model. If a model cannot generalize well to new data, then it cannot be leveraged for classification or prediction tasks. Generalization of a model to new data is ultimately what allows us to use machine learning algorithms every day to make predictions and classify data.\n",
    "\n",
    "High bias and low variance are good indicators of underfitting. Since this behavior can be seen while using the training dataset, underfitted models are usually easier to identify than overfitted ones.\n",
    "\n",
    "## Underfitting vs. Overfitting\n",
    "Put simply, overfitting is the opposite of underfitting, occurring when the model has been overtrained or when it contains too much complexity, resulting in high error rates on test data. Overfitting a model is more common than underfitting one, and underfitting typically occurs in an effort to avoid overfitting through a process called “early stopping.”\n",
    "\n",
    "If undertraining or lack of complexity results in underfitting, then a logical prevention strategy would be to increase the duration of training or add more relevant inputs. However, if you train the model too much or add too many features to it, you may overfit your model, resulting in low bias but high variance (i.e. the bias-variance tradeoff). In this scenario, the statistical model fits too closely against its training data, rendering it unable to generalize well to new data points. It's important to note that some types of models can be more prone to overfitting than others, such as decision trees or KNN.  \n",
    "\n",
    "\n",
    "## How to avoid underfitting\n",
    "Since we can detect underfitting based off of the training set, we can better assist at establishing the dominant relationship between the input and output variables at the onset. By maintaining adequate model complexity, we can avoid underfitting and make more accurate predictions. Below are a few techniques that can be used to reduce underfitting:\n",
    "\n",
    "Decrease regularization\n",
    "Regularization is typically used to reduce the variance with a model by applying a penalty to the input parameters with the larger coefficients. There are a number of different methods, such as L1 regularization, Lasso regularization, dropout, etc., which help to reduce the noise and outliers within a model. However, if the data features become too uniform, the model is unable to identify the dominant trend, leading to underfitting. By decreasing the amount of regularization, more complexity and variation is introduced into the model, allowing for successful training of the model.\n",
    "\n",
    "Increase the duration of training\n",
    "As mentioned earlier, stopping training too soon can also result in underfit model. Therefore, by extending the duration of training, it can be avoided. However, it is important to cognizant of overtraining, and subsequently, overfitting. Finding the balance between the two scenarios will be key.\n",
    "\n",
    "Feature selection\n",
    "With any model, specific features are used to determine a given outcome. If there are not enough predictive features present, then more features or features with greater importance, should be introduced. For example, in a neural network, you might add more hidden neurons or in a random forest, you may add more trees. This process will inject more complexity into the model, yielding better training results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw how an underfitting model simply did not learn from the data while an overfitting one actually learned the data almost by heart and therefore failed to generalize to new data.\n",
    "\n",
    "Now,  how do you solve these problems?\n",
    "\n",
    "Let’s start with the most common and complex problem: overfitting. There are 4 main techniques you can try:\n",
    "\n",
    "Adding more data\n",
    "\n",
    "Your model is overfitting when it fails to generalize to new data. That means the data it was trained on is not representative of the data it is meeting in production. So, retraining your algorithm on a bigger, richer and more diverse data set should improve its performance. Unfortunately, getting more data can prove to be very difficult; either because collecting it is very expensive or because very few samples are regularly generated. In that case, it might be a good idea to use data augmentation.\n",
    "\n",
    "Data augmentation\n",
    "\n",
    "This is a set of techniques used to artificially increase the size of a dataset by applying transformations to the existing data. For instance, in the case of images, you can flip images horizontally or vertically, crop them or rotate them. You can also turn them into grayscale or change the color saturation.  As far as the algorithm is concerned, new data has been created. Of course, not all transformations are useful in every case. And in some cases, your algorithm won’t be fooled… In short, data augmentation can be a very powerful tool but it requires a careful examination and understanding of your data. \n",
    "\n",
    "Regularization\n",
    "\n",
    "Regularization actually refers to a large range of techniques and we won’t list them all or go into details here. The main idea you need to remember is that these techniques introduce a “complexity penalty” to your model. If the model wants to avoid incurring that penalty, it needs to focus on the most prominent patterns which have a better chance of generalizing well. Regularization techniques are very powerful and almost all the models you will build will use them in some way. \n",
    "\n",
    "Removing features from data\n",
    "\n",
    "Sometimes, your model may fail to generalize simply because the data it was trained on was too complex and the model missed the patterns it should have detected. Removing some features and making your data simpler can help reduce overfitting.\n",
    "\n",
    "It is important to understand that overfitting is a complex problem. You will almost systematically face it when you develop a deep learning model and you should not get discouraged if you are struggling to address it. Even the most experienced ML engineers spend a lot of time trying to solve it.\n",
    "\n",
    "Let’s now switch to the problem of underfitting. Here is what you can try:\n",
    "\n",
    "Increasing the model complexity\n",
    "\n",
    "Your model may be underfitting simply because it is not complex enough to capture patterns in the data. Using a more complex model, for instance by switching from a linear to a non-linear model or by adding hidden layers to your neural network, will very often help solve underfitting.\n",
    "\n",
    "Reducing regularization\n",
    "\n",
    "The algorithms you use include by default regularization parameters meant to prevent overfitting. Sometimes, they prevent the algorithm from learning. Reducing their values generally helps.\n",
    "\n",
    "Adding features to training data\n",
    "\n",
    "In contrast to overfitting, your model may be underfitting because the training data is too simple. It may lack the features that will make the model detect the relevant patterns to make accurate predictions. Adding features and complexity to your data can help overcome underfitting.\n",
    "\n",
    "Did you notice? That’s right! Adding more data is not included in the techniques to solve underfitting. Indeed, if your data is lacking the decisive features to allow your model to detect patterns, you can multiply your training set size by 2, 5 or even 10, it won’t make your algorithm better! \n",
    "\n",
    "Unfortunately, it has become a reflex in the industry. No matter what the problem their model is facing, a lot of engineers think that throwing more data at it will solve the problem. When you know how time-consuming and expensive it can be to collect data, this is a mistake that can seriously harm or even jeopardize a project. \n",
    "\n",
    "Being able to diagnose and tackle underfitting/overfitting is an essential part of the process of developing a good model. Of course, there are lots of other techniques to solve these problems on top of the ones we just listed. But these are the most important ones and if you manage to master them, you will be well equipped to start your machine learning journey!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Simple Techniques to Prevent Overfitting\n",
    "\n",
    "\n",
    "Table of Contents\n",
    "1. Hold-out\n",
    "2. Cross-validation\n",
    "3. Data augmentation\n",
    "4. Feature selection\n",
    "5. L1 / L2 regularization\n",
    "6. Remove layers / number of units per layer\n",
    "7. Dropout\n",
    "8. Early stopping\n",
    "\n",
    "\n",
    "1. Hold-out (data)\n",
    "\n",
    "Rather than using all of our data for training, we can simply split our dataset into two sets: training and testing. A common split ratio is 80% for training and 20% for testing. We train our model until it performs well not only on the training set but also for the testing set. This indicates good generalization capability since the testing set represents unseen data that were not used for training. However, this approach would require a sufficiently large dataset to train on even after splitting.\n",
    "\n",
    "2. Cross-validation (data)\n",
    "\n",
    "We can split our dataset into k groups (k-fold cross-validation). We let one of the groups to be the testing set (please see hold-out explanation) and the others as the training set, and repeat this process until each individual group has been used as the testing set (e.g., k repeats). Unlike hold-out, cross-validation allows all data to be eventually used for training but is also more computationally expensive than hold-out.\n",
    "\n",
    "<img src=\"./i/0_jLUbzKXfGhZWG25A.png\" />\n",
    "\n",
    "3. Data augmentation (data)\n",
    "\n",
    "A larger dataset would reduce overfitting. If we cannot gather more data and are constrained to the data we have in our current dataset, we can apply data augmentation to artificially increase the size of our dataset. For example, if we are training for an image classification task, we can perform various image transformations to our image dataset (e.g., flipping, rotating, rescaling, shifting).\n",
    "<img src=\"./i/0_JgP_DG16kisBAdpS.png\" />\n",
    "\n",
    "\n",
    "4. Feature selection (data)\n",
    "\n",
    "If we have only a limited amount of training samples, each with a large number of features, we should only select the most important features for training so that our model doesn’t need to learn for so many features and eventually overfit. We can simply test out different features, train individual models for these features, and evaluate generalization capabilities, or use one of the various widely used feature selection methods.\n",
    "\n",
    "<img src=\"./i/0_N3paES6IzJ8oyh9p.png\" />\n",
    "\n",
    "5. L1 / L2 regularization (learning algorithm)\n",
    "\n",
    "Regularization is a technique to constrain our network from learning a model that is too complex, which may therefore overfit. In L1 or L2 regularization, we can add a penalty term on the cost function to push the estimated coefficients towards zero (and not take more extreme values). L2 regularization allows weights to decay towards zero but not to zero, while L1 regularization allows weights to decay to zero.\n",
    "\n",
    "<img src=\"./i/0_69Jgv2gwAPtOIwNh.png\" />\n",
    "\n",
    "6. Remove layers / number of units per layer (model)\n",
    "As mentioned in L1 or L2 regularization, an over-complex model may more likely overfit. Therefore, we can directly reduce the model’s complexity by removing layers and reduce the size of our model. We may further reduce complexity by decreasing the number of neurons in the fully-connected layers. We should have a model with a complexity that sufficiently balances between underfitting and overfitting for our task.\n",
    "\n",
    "<img src=\"./i/0_9RwXpjZWOrcEdhQ7.png\" />\n",
    "\n",
    "7. Dropout (model)\n",
    "\n",
    "By applying dropout, which is a form of regularization, to our layers, we ignore a subset of units of our network with a set probability. Using dropout, we can reduce interdependent learning among units, which may have led to overfitting. However, with dropout, we would need more epochs for our model to converge.\n",
    "\n",
    "<img src=\"./i/0_YCofAkhSErYvlpRT.png\" />\n",
    "\n",
    "\n",
    "\n",
    "8. Early stopping (model)\n",
    "We can first train our model for an arbitrarily large number of epochs and plot the validation loss graph (e.g., using hold-out). Once the validation loss begins to degrade (e.g., stops decreasing but rather begins increasing), we stop the training and save the current model. We can implement this either by monitoring the loss graph or set an early stopping trigger. The saved model would be the optimal model for generalization among different training epoch values.\n",
    "\n",
    "<img src=\"./i/0_b4lf4K0PswVYZXdI.png\" />\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
